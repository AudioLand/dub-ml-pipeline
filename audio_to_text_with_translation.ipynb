{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Speech to text"
      ],
      "metadata": {
        "id": "xlniOzAkxESo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installs & imports"
      ],
      "metadata": {
        "id": "bhCQPJ6pxESq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qhl-Dy2MxESr"
      },
      "outputs": [],
      "source": [
        "!pip install -U openai-whisper\n",
        "!pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\n",
        "!pip install ffmpeg\n",
        "!pip install setuptools-rust\n",
        "\n",
        "!pip install pytube\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install openai\n",
        "!pip install pydub\n",
        "!pip install elevenlabs\n",
        "\n",
        "!pip install tempfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import torch\n",
        "\n",
        "from pytube import YouTube\n",
        "# from pprint import pprint\n",
        "import tempfile\n",
        "import openai\n",
        "from pydub import AudioSegment\n",
        "\n",
        "import os\n",
        "import tempfile"
      ],
      "metadata": {
        "id": "zSzl7h6bxESt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OpenAI API Whisper (файлы до 25MB -- upd: теперь больше)\n"
      ],
      "metadata": {
        "id": "mYm6HWP9J8_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "openai.api_key = \"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\""
      ],
      "metadata": {
        "id": "HNu6csOED-JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def youtubeDownload(link):\n",
        "  yt = YouTube(link)\n",
        "  video = yt.streams.get_highest_resolution().download()\n",
        "  return video\n",
        "\n",
        "def speechToText(video_path):\n",
        "  song = AudioSegment.from_file(video_path)\n",
        "  # PyDub handles time in milliseconds\n",
        "  ten_minutes = 10 * 60 * 1000\n",
        "  transcript =[]\n",
        "  for start_moment in range(0, len(song), ten_minutes):\n",
        "    finish_moment = len(song) if start_moment+ten_minutes>len(song) else start_moment+ten_minutes\n",
        "    current_10_minutes = song[start_moment:finish_moment]\n",
        "    one_piece = current_10_minutes.export(\"filename.mp4\", format=\"mp4\")\n",
        "    transcript.append(openai.Audio.translate(\"whisper-1\", one_piece)['text'])\n",
        "  return ' '.join(transcript)"
      ],
      "metadata": {
        "id": "7GM0oG7_7J1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Slava's version (more readable code + improved error handling)\n",
        "\n",
        "def speechToText(video_path):\n",
        "    \"\"\"Convert the audio content of a video into text.\"\"\"\n",
        "    try:\n",
        "        # Check if the file exists\n",
        "        if not os.path.exists(video_path):\n",
        "            raise ValueError(f\"File not found: {video_path}\")\n",
        "\n",
        "        # Get file extension for format\n",
        "        file_format = os.path.splitext(video_path)[-1].replace(\".\", \"\")\n",
        "\n",
        "        # Extract Audio from Video\n",
        "        audio_content = AudioSegment.from_file(video_path, format=file_format)\n",
        "\n",
        "        # Determine the 10-minute Mark\n",
        "        ten_minutes_in_ms = 10 * 60 * 1000\n",
        "\n",
        "        # Initialize an Empty Transcript\n",
        "        transcript_parts = []\n",
        "\n",
        "        # Loop Through 10-minute Segments\n",
        "        for start_time in range(0, len(audio_content), ten_minutes_in_ms):\n",
        "            end_time = min(len(audio_content), start_time + ten_minutes_in_ms)\n",
        "            audio_segment = audio_content[start_time:end_time]\n",
        "\n",
        "            # Use a temporary file to avoid overwriting conflicts\n",
        "            with tempfile.NamedTemporaryFile(delete=True, suffix=\".wav\") as temp_file:\n",
        "                audio_segment.export(temp_file.name, format=\"wav\")\n",
        "\n",
        "                # Use OpenAI's Whisper ASR to transcribe\n",
        "                with open(temp_file.name, \"rb\") as audio_file:\n",
        "                    transcript_parts.append(openai.Audio.translate(\"whisper-1\", audio_file)['text'])\n",
        "\n",
        "        # Concatenate and Return the Transcription\n",
        "        return ' '.join(transcript_parts)\n",
        "\n",
        "    except ValueError as ve:\n",
        "        print(f\"ValueError: {str(ve)}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        # Handle generic exceptions and provide feedback\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "XdlHbht5ueat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text translation"
      ],
      "metadata": {
        "id": "JH8tetofws-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"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\""
      ],
      "metadata": {
        "id": "MGmWmSAQE841"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes lanuguage to translate on and text\n",
        "# Returns translated text.\n",
        "def text_translation(language, text):\n",
        "  chat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
        "                               messages=[{\"role\": \"user\", \"content\": f\"Translate the below text in {language}. Text: {text} \\\n",
        "                               If the text is already in {language}, just write this text in the answer without translation.\\\n",
        "                               If you are not able to translate this text, also write this text in the answer without translation.\\\n",
        "                               Start your answer with the translated text. Your answer:\"}])\n",
        "  print(language + \":\")\n",
        "  print(chat_completion['choices'][0]['message']['content'])\n",
        "  print()\n",
        "  return chat_completion['choices'][0]['message']['content']"
      ],
      "metadata": {
        "id": "uwpFIr6TcaDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Slava's code\n",
        "\n",
        "def translate_text(language, text):\n",
        "    \"\"\"\n",
        "    Translates a given text into the specified language using OpenAI's model.\n",
        "\n",
        "    Parameters:\n",
        "    - language (str): The target language for translation.\n",
        "    - text (str): The text to be translated.\n",
        "\n",
        "    Returns:\n",
        "    - str: Translated text or original text if translation is not possible.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = (f\"Translate the below text in {language}. Text: {text} \"\n",
        "              f\"If the text is already in {language}, just write this text in the answer without translation. \"\n",
        "              f\"If you are not able to translate this text, also write this text in the answer without translation. \"\n",
        "              f\"Start your answer with the translated text. Your answer:\")\n",
        "\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        translated_text = response['choices'][0]['message']['content']\n",
        "        return translated_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during translation: {e}\")\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "djEZKH3PJA2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Voice Gender Detection"
      ],
      "metadata": {
        "id": "3yKJuf2rkOVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "fVACYlWQGS9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_URL = \"https://api-inference.huggingface.co/models/alefiury/wav2vec2-large-xlsr-53-gender-recognition-librispeech\"\n",
        "headers = {\"Authorization\": f\"Bearer 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\"}"
      ],
      "metadata": {
        "id": "GNcQIvjmGX9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes the filename of the audio with the voice\n",
        "# Returns the voice gender ('female' or 'male'), str type\n",
        "def voice_gender_detection(filename):\n",
        "    with open(filename, \"rb\") as f:\n",
        "        data = f.read()\n",
        "    response = requests.post(API_URL, headers=headers, data=data)\n",
        "    return response.json()[0]['label']"
      ],
      "metadata": {
        "id": "k11xBvN-kNUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text to speech"
      ],
      "metadata": {
        "id": "5nhoBEeq-98X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from elevenlabs import generate, play, set_api_key, clone\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "wjJrLCIyxFSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: text, gender, is_video - boolean, if true video_path\n",
        "# Output: audio / video file\n",
        "\n",
        "def text_to_speech(text, detected_gender, is_video, video_path = None):\n",
        "  if detected_gender == 'female':\n",
        "    audio = generate(\n",
        "      text = text,\n",
        "      voice = \"Rachel\",\n",
        "      model = \"eleven_multilingual_v2\"\n",
        "    )\n",
        "  else:\n",
        "    audio = generate(\n",
        "      text = text,\n",
        "      voice = \"Josh\",\n",
        "      model = \"eleven_multilingual_v2\"\n",
        "    )\n",
        "\n",
        "  # if is_video:\n",
        "\n",
        "\n",
        "  with open('new_audio.mp3', mode='bx') as f:\n",
        "    f.write(audio)\n",
        "\n",
        "  # Download file\n",
        "  files.download('new_audio.mp3')\n",
        "\n",
        "  return audio"
      ],
      "metadata": {
        "id": "AEWjSluG--MG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}