# -*- coding: utf-8 -*-
"""audio_to_text_with_translation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EptXlGBi4IeiKs7q4ehbQE_cIoECv4nG

## Speech to text

### Installs & imports
"""

# !pip install -U openai-whisper
# !pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git
# !pip install ffmpeg
# !pip install setuptools-rust
#
# !pip install pytube
# !pip install torch torchvision torchaudio
# !pip install openai
# !pip install pydub
# !pip install elevenlabs
#
# !pip install tempfile

import whisper
import torch

from pytube import YouTube
# from pprint import pprint
import tempfile
import openai
from pydub import AudioSegment

import os
import tempfile

"""#### OpenAI API Whisper (файлы до 25MB -- upd: теперь больше)

"""

device = 'cuda' if torch.cuda.is_available() else 'cpu'

openai.api_key = "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"

def youtubeDownload(link):
  yt = YouTube(link)
  video = yt.streams.get_highest_resolution().download()
  return video

def speechToText(video_path):
    """Convert the audio content of a video into text."""
    try:
        # Check if the file exists
        if not os.path.exists(video_path):
            raise ValueError(f"File not found: {video_path}")

        # Get file extension for format
        file_format = os.path.splitext(video_path)[-1].replace(".", "")

        # Extract Audio from Video
        audio_content = AudioSegment.from_file(video_path, format=file_format)

        # Determine the 10-minute Mark
        ten_minutes_in_ms = 10 * 60 * 1000

        # Initialize an Empty Transcript
        transcript_parts = []

        # Loop Through 10-minute Segments
        for start_time in range(0, len(audio_content), ten_minutes_in_ms):
            end_time = min(len(audio_content), start_time + ten_minutes_in_ms)
            audio_segment = audio_content[start_time:end_time]

            # Use a temporary file to avoid overwriting conflicts
            with tempfile.NamedTemporaryFile(delete=True, suffix=".wav") as temp_file:
                audio_segment.export(temp_file.name, format="wav")

                # Use OpenAI's Whisper ASR to transcribe
                with open(temp_file.name, "rb") as audio_file:
                    transcript_parts.append(openai.Audio.translate("whisper-1", audio_file)['text'])

        # Concatenate and Return the Transcription
        return ' '.join(transcript_parts)

    except ValueError as ve:
        print(f"ValueError: {str(ve)}")
        return None
    except Exception as e:
        # Handle generic exceptions and provide feedback
        print(f"An error occurred: {str(e)}")
        return None

"""## Text translation"""

openai.api_key = "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"

def translate_text(language, text):
    """
    Translates a given text into the specified language using OpenAI's model.

    Parameters:
    - language (str): The target language for translation.
    - text (str): The text to be translated.

    Returns:
    - str: Translated text or original text if translation is not possible.
    """

    prompt = (f"Translate the below text in {language}. Text: {text} "
              f"If the text is already in {language}, just write this text in the answer without translation. "
              f"If you are not able to translate this text, also write this text in the answer without translation. "
              f"Start your answer with the translated text. Your answer:")

    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        translated_text = response['choices'][0]['message']['content']
        return translated_text

    except Exception as e:
        print(f"Error during translation: {e}")
        return text

"""### Voice Gender Detection"""

import requests

API_URL = "https://api-inference.huggingface.co/models/alefiury/wav2vec2-large-xlsr-53-gender-recognition-librispeech"
headers = {"Authorization": f"Bearer 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"}

# Takes the filename of the audio with the voice
# Returns the voice gender ('female' or 'male'), str type
def voice_gender_detection(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.post(API_URL, headers=headers, data=data)
    return response.json()[0]['label']

"""## Text to speech"""

from elevenlabs import generate, play, set_api_key, clone
from google.colab import files

# Input: text, gender, is_video - boolean, if true video_path
# Output: audio / video file

def text_to_speech(text, detected_gender, is_video, video_path = None):
  if detected_gender == 'female':
    audio = generate(
      text = text,
      voice = "Rachel",
      model = "eleven_multilingual_v2"
    )
  else:
    audio = generate(
      text = text,
      voice = "Josh",
      model = "eleven_multilingual_v2"
    )

  # if is_video:


  with open('new_audio.mp3', mode='bx') as f:
    f.write(audio)

  # Download file
  files.download('new_audio.mp3')

  return audio